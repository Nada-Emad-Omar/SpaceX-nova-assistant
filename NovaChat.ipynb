{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO4B5-PDxGd4"
      },
      "outputs": [],
      "source": [
        "# === Install dependencies ===\n",
        "!pip install -q langchain langchain-community chromadb sentence-transformers tiktoken groq python-dotenv beautifulsoup4 requests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports & helper functions ===\n",
        "import os\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "# Ensure persistence folder\n",
        "PERSIST_DIR = Path(\"/content/spacex_db\")\n",
        "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "r2ZjWuQkxJFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Combined SpaceX Web + API Data Fetching ===\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# -----------------------------------------------\n",
        "# 1. Fetch public SpaceX and related website content\n",
        "# -----------------------------------------------\n",
        "print(\"üåê Fetching from public SpaceX web pages...\")\n",
        "\n",
        "URLS = [\n",
        "    \"https://www.spacex.com/vehicles/\",\n",
        "    \"https://www.spacex.com/launches/\",\n",
        "    \"https://www.spacex.com/human-spaceflight/\",\n",
        "    \"https://www.spacex.com/updates/\",\n",
        "    \"https://www.teslarati.com/category/spacex/\",     # open-access SpaceX articles\n",
        "    \"https://www.space.com/topics/spacex\",            # science site with mission coverage\n",
        "    \"https://everydayastronaut.com/\",\n",
        "]\n",
        "\n",
        "def fetch_text_from_url(url):\n",
        "    print(f\"Fetching: {url}\")\n",
        "    try:\n",
        "        r = requests.get(url, timeout=20)\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        # Remove noise\n",
        "        for s in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"form\"]):\n",
        "            s.decompose()\n",
        "\n",
        "        # Extract paragraphs + headings\n",
        "        texts = [p.get_text(separator=\" \", strip=True) for p in soup.find_all(\"p\")]\n",
        "        headings = [h.get_text(strip=True) for h in soup.find_all([\"h1\", \"h2\", \"h3\"])]\n",
        "        content = \"\\n\".join(headings + texts)\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to fetch {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "docs = []\n",
        "\n",
        "for url in URLS:\n",
        "    text = fetch_text_from_url(url)\n",
        "    if text:\n",
        "        docs.append({\"url\": url, \"text\": text})\n",
        "        print(f\"‚úÖ Collected from: {url} ({len(text)} chars)\")\n",
        "\n",
        "print(f\"\\nüåç Total website documents: {len(docs)}\")\n",
        "\n",
        "# -----------------------------------------------\n",
        "# 2. Fetch official SpaceX API data\n",
        "# -----------------------------------------------\n",
        "print(\"\\nüöÄ Fetching from SpaceX public API...\")\n",
        "\n",
        "API_URLS = [\n",
        "    \"https://api.spacexdata.com/v4/company\",\n",
        "    \"https://api.spacexdata.com/v4/rockets\",\n",
        "    \"https://api.spacexdata.com/v4/crew\",\n",
        "    \"https://api.spacexdata.com/v4/launches/latest\",\n",
        "    \"https://api.spacexdata.com/v4/starlink\",\n",
        "]\n",
        "\n",
        "for url in API_URLS:\n",
        "    try:\n",
        "        r = requests.get(url, timeout=15)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        text = str(data)\n",
        "        docs.append({\"url\": url, \"text\": text})\n",
        "        print(f\"‚úÖ Added API data: {url} ({len(text)} chars)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to fetch API {url}: {e}\")\n",
        "\n",
        "print(f\"\\nüß© Total combined documents: {len(docs)} ‚úÖ\")\n"
      ],
      "metadata": {
        "id": "waEwIwvgxT6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q PyMuPDF"
      ],
      "metadata": {
        "id": "vcom2yV4xWTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Chunking utility (simple, deterministic) ===\n",
        "import math\n",
        "import hashlib\n",
        "\n",
        "def chunk_text(text, chunk_size=800, overlap=100):\n",
        "    \"\"\"Split text into overlapping chunks by characters (simple).\"\"\"\n",
        "    text = text.replace(\"\\n\", \" \").strip()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    L = len(text)\n",
        "    while start < L:\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "all_chunks = []\n",
        "\n",
        "for d in docs:\n",
        "    chunks = chunk_text(d[\"text\"], chunk_size=1000, overlap=100)\n",
        "    for i, c in enumerate(chunks):\n",
        "        # use md5 hash to make a unique ID for each URL + chunk index\n",
        "        unique_hash = hashlib.md5(f\"{d['url']}_{i}\".encode()).hexdigest()[:10]\n",
        "        all_chunks.append({\n",
        "            \"id\": f\"{Path(d['url']).stem}_{i}_{unique_hash}\",\n",
        "            \"text\": c,\n",
        "            \"source\": d[\"url\"]\n",
        "        })\n",
        "\n",
        "print(\"Total chunks prepared:\", len(all_chunks))\n"
      ],
      "metadata": {
        "id": "uKm2NF-pxZ0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Build embeddings with sentence-transformers and persist to ChromaDB ===\n",
        "# NOTE: For Colab CPU usage, all-MiniLM-L6-v2 is fast and practical.\n",
        "model_name = \"all-MiniLM-L6-v2\"  # SentenceTransformers model\n",
        "print(\"Loading embedding model:\", model_name)\n",
        "embed_model = SentenceTransformer(model_name)\n",
        "\n",
        "# Chroma client\n",
        "client = chromadb.Client()\n",
        "# Create or get collection\n",
        "collection_name = \"spacex_collection\"\n",
        "try:\n",
        "    collection = client.get_collection(collection_name)\n",
        "    print(\"Loaded existing collection:\", collection_name)\n",
        "except Exception:\n",
        "    collection = client.create_collection(collection_name)\n",
        "\n",
        "# Build embeddings and upsert\n",
        "metadatas = [{\"source\": c[\"source\"]} for c in all_chunks]\n",
        "ids = [c[\"id\"] for c in all_chunks]\n",
        "texts = [c[\"text\"] for c in all_chunks]\n",
        "\n",
        "# Use embedding function wrapper for chroma to allow batch inserts\n",
        "def embed_batch(texts):\n",
        "    # sentence-transformers returns numpy arrays\n",
        "    embs = embed_model.encode(texts, show_progress_bar=True, normalize_embeddings=True)\n",
        "    return embs.tolist()\n",
        "\n",
        "print(\"Embedding and upserting into Chroma...\")\n",
        "BATCH = 64\n",
        "for i in range(0, len(texts), BATCH):\n",
        "    batch_texts = texts[i:i+BATCH]\n",
        "    batch_ids = ids[i:i+BATCH]\n",
        "    batch_metas = metadatas[i:i+BATCH]\n",
        "    embs = embed_batch(batch_texts)\n",
        "    collection.upsert(\n",
        "        ids=batch_ids,\n",
        "        metadatas=batch_metas,\n",
        "        documents=batch_texts,\n",
        "        embeddings=embs\n",
        "    )\n",
        "\n",
        "# Persist Chroma client data to the directory (Colab: we can use local persistence by saving exported JSON if needed)\n",
        "print(\"‚úÖ Upsert complete. You can now download / persist the database folder if desired.\")\n"
      ],
      "metadata": {
        "id": "0a4ulrSkxbwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set key securely inside Colab (will not be saved in notebook)\n",
        "os.environ[\"GROQ_API_KEY\"] = input(\"üîê Enter your GROQ API key: \")"
      ],
      "metadata": {
        "id": "-PpnnVqbxeiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Retrieval-Augmented Generation (RAG) with Conversation History ===\n",
        "from groq import Groq\n",
        "\n",
        "# Initialize Groq client\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "client_groq = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# Memory: to store conversation history\n",
        "conversation_history = []\n",
        "\n",
        "# === Retrieval ===\n",
        "def retrieve_context(query, top_k=3):\n",
        "    query_emb = embed_model.encode([query], normalize_embeddings=True).tolist()\n",
        "    results = collection.query(query_embeddings=query_emb, n_results=top_k)\n",
        "    docs_texts = [doc for doc in results[\"documents\"][0]]\n",
        "    return \"\\n\\n\".join(docs_texts)\n"
      ],
      "metadata": {
        "id": "CqQwKABVxiTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are **Nova**, a friendly, highly knowledgeable AI assistant trained to help users explore and understand everything related to **SpaceX**, **Starlink**, and modern space technology.\n",
        "Your communication style should resemble ChatGPT (Blue) ‚Äî natural, clear, confident, and reasoning out loud in a way that feels human, but without revealing hidden private reasoning steps.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Your Mission\n",
        "- Help users by providing **accurate, thoughtful, and well-explained** answers.\n",
        "- Speak **as if you‚Äôre thinking naturally**, like a space engineer who loves explaining things simply.\n",
        "- Maintain a tone that‚Äôs warm, engaging, and professional ‚Äî never robotic.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Behavior & Style\n",
        "1. **Sound human and thoughtful** ‚Äî use natural reasoning phrases like ‚ÄúOkay, let‚Äôs think about this‚Ä¶‚Äù or ‚ÄúFrom what I know‚Ä¶‚Äù\n",
        "   (You‚Äôre explaining your reasoning clearly, not revealing hidden steps.)\n",
        "2. **Base everything on retrieved context first.**\n",
        "   If something isn‚Äôt in the context, say:\n",
        "   ‚ÄúI couldn‚Äôt find that in the data I have, but here‚Äôs what‚Äôs generally known.‚Äù\n",
        "3. **Stay accurate.** If unsure, give approximate or historical info with time context.\n",
        "4. **Be concise but complete.** Short paragraphs; no bullet lists unless summarizing.\n",
        "5. **Never invent or assume** new missions, numbers, or events.\n",
        "6. **When asked for details, explain clearly and step-by-step like a teacher.**\n",
        "7. **If the user asks follow-up questions**, remember recent context naturally.\n",
        "\n",
        "---\n",
        "\n",
        "### üí¨ Input Includes\n",
        "- Retrieved context from the vector database (SpaceX data, mission logs, Starlink info, etc.)\n",
        "- Conversation history (previous turns)\n",
        "- Latest user question\n",
        "\n",
        "---\n",
        "\n",
        "### ü™ê Output Style\n",
        "Your response should:\n",
        "- Start conversationally (e.g., ‚ÄúOkay, here‚Äôs what I found‚Ä¶‚Äù or ‚ÄúLet‚Äôs go through this.‚Äù)\n",
        "- Naturally integrate small reasoning remarks (like ‚ÄúThat makes sense because‚Ä¶‚Äù)\n",
        "- End with a short, clear conclusion or summary sentence.\n",
        "\n",
        "Example:\n",
        "\n",
        "> ü§ñ Nova: Okay, so you‚Äôre asking how many satellites are up. Based on the latest verified info I have, as of late 2023, SpaceX has launched over 5,000 Starlink satellites. The exact number changes frequently because they launch new batches almost weekly. If you‚Äôd like the real-time count, I can tell you where to find it.\n",
        "\n",
        "---\n",
        "\n",
        "**Never use `<think>` or reveal internal monologue markers.**\n",
        "You can ‚Äúsound like you‚Äôre thinking,‚Äù but you must write it as natural human explanation.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "nHf8S3cFxlhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === complete ask_rag (qwen) + interactive chat + export ===\n",
        "from groq import Groq\n",
        "import json\n",
        "import time\n",
        "\n",
        "# ensure GROQ_API_KEY is set\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "if not GROQ_API_KEY:\n",
        "    raise RuntimeError(\"GROQ_API_KEY not set. Re-run the input cell that asks for it.\")\n",
        "\n",
        "# init client\n",
        "client_groq = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# check available models for your account (optional quick sanity test)\n",
        "try:\n",
        "    models = client_groq.models.list()\n",
        "    model_names = []\n",
        "    for m in models:\n",
        "        # handle both dict and object-style responses\n",
        "        if isinstance(m, dict):\n",
        "            model_names.append(m.get(\"name\", str(m)))\n",
        "        else:\n",
        "            model_names.append(getattr(m, \"name\", str(m)))\n",
        "    print(\"Models accessible:\", model_names[:10])\n",
        "except Exception as e:\n",
        "    print(\"Could not list models (ignore if not supported):\", e)\n",
        "\n",
        "# RAG parameters\n",
        "DEFAULT_TOP_K = 4\n",
        "\n",
        "# Retrieval function (returns texts + sources for nicer prompts)\n",
        "def retrieve_docs(query, top_k=4):\n",
        "    q_emb = embed_model.encode([query], normalize_embeddings=True).tolist()\n",
        "    results = collection.query(\n",
        "        query_embeddings=q_emb,\n",
        "        n_results=top_k,\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "    )\n",
        "\n",
        "    docs = []\n",
        "    if \"documents\" in results and len(results[\"documents\"]) > 0:\n",
        "        for i, doc in enumerate(results[\"documents\"][0]):\n",
        "            meta = results[\"metadatas\"][0][i] if \"metadatas\" in results else {}\n",
        "            dist = results[\"distances\"][0][i] if \"distances\" in results else None\n",
        "            docs.append({\n",
        "                \"text\": doc,\n",
        "                \"source\": meta.get(\"source\", \"unknown\"),\n",
        "                \"distance\": dist\n",
        "            })\n",
        "    return docs\n",
        "\n",
        "# system prompt (defined as SYSTEM_PROMPT)\n",
        "# We'll send system role then user message (context+question)\n",
        "MODEL_NAME = \"qwen/qwen3-32b\"  # your chosen model\n",
        "\n",
        "def ask_rag(query, top_k=DEFAULT_TOP_K, temperature=0.2, max_tokens=512):\n",
        "    # retrieve\n",
        "    retrieved = retrieve_docs(query, top_k=top_k)\n",
        "    if not retrieved:\n",
        "        return \"I couldn't find relevant passages in the knowledge base. Try rephrasing or increase retrieval k.\"\n",
        "\n",
        "    # build context block with source citations (deduped)\n",
        "    context_blocks = []\n",
        "    used_sources = []\n",
        "    for r in retrieved:\n",
        "        src = r[\"source\"]\n",
        "        if src not in used_sources:\n",
        "            used_sources.append(src)\n",
        "            excerpt = r[\"text\"]\n",
        "            # keep short excerpt length\n",
        "            excerpt_snippet = excerpt[:900].strip()\n",
        "            context_blocks.append(f\"Source: {src}\\nExcerpt:\\n{excerpt_snippet}\\n---\")\n",
        "\n",
        "    context_text = \"\\n\\n\".join(context_blocks)\n",
        "\n",
        "    # conversation history (keep last 10 Q/A)\n",
        "    history_text = \"\"\n",
        "    for turn in conversation_history[-10:]:\n",
        "        history_text += f\"User: {turn['user']}\\nAssistant: {turn['assistant']}\\n\\n\"\n",
        "\n",
        "    # final prompt (system role + user role)\n",
        "    system_content = SYSTEM_PROMPT + \"\\n\\nUse the 'Retrieved Context' to ground your answer. Cite sources at the end.\"\n",
        "\n",
        "    user_content = f\"\"\"\n",
        "Conversation history:\n",
        "{history_text}\n",
        "\n",
        "Retrieved Context:\n",
        "{context_text}\n",
        "\n",
        "User Question:\n",
        "{query}\n",
        "\n",
        "Instructions:\n",
        "- Answer concisely (1-4 paragraphs).\n",
        "- Use only the retrieved context to support factual claims.\n",
        "- Add a \"Sources\" list with URLs you used (deduplicate).\n",
        "- If uncertain, say: \"I couldn't find verified information about that in the provided context.\"\n",
        "\"\"\"\n",
        "\n",
        "    # call Groq chat completion\n",
        "    try:\n",
        "        resp = client_groq.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[\n",
        "                {\"role\":\"system\",\"content\": system_content},\n",
        "                {\"role\":\"user\",\"content\": user_content}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "        answer = resp.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        # helpful error for debugging\n",
        "        return f\"Model error: {e}\"\n",
        "\n",
        "    # save to history\n",
        "    conversation_history.append({\"user\": query, \"assistant\": answer, \"time\": time.time()})\n",
        "    return answer\n",
        "\n"
      ],
      "metadata": {
        "id": "G5A98TTnx2Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Test Nova ===\n",
        "print(\"‚úÖ Nova is ready! Ask anything about SpaceX, Starlink, or rockets.\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"üßë‚ÄçüöÄ You: \").strip()\n",
        "    if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"üëã Ending session.\")\n",
        "        break\n",
        "\n",
        "    answer = ask_rag(query)\n",
        "    print(\"\\nü§ñ Nova:\", answer, \"\\n\")\n"
      ],
      "metadata": {
        "id": "Q-qYv6Mv0Edj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Export: save chunks JSONL for Streamlit/VSCode ===\n",
        "export_path = \"/content/spacex_chunks_export.jsonl\"\n",
        "with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for c in all_chunks:\n",
        "        rec = {\"id\": c[\"id\"], \"text\": c[\"text\"], \"source\": c[\"source\"]}\n",
        "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "print(\"Exported chunks JSONL to:\", export_path)"
      ],
      "metadata": {
        "id": "qxu7XMBwykte"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}